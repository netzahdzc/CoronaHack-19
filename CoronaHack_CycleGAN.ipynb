{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoronaHack (Cycle GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to remove all variables and clean cache memory\n",
    "from IPython import get_ipython\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(image_type, image_dir='../../../datasets/covid_19/augmentation_gan/', \n",
    "                    image_size=224, batch_size=64, num_workers=0):\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize(image_size), \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "    image_path = './' + image_dir\n",
    "    train_path = os.path.join(image_path, image_type)\n",
    "    test_path = os.path.join(image_path, 'test_{}'.format(image_type))\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_X, test_dataloader_X = get_data_loader(image_type='normal')\n",
    "dataloader_Y, test_dataloader_Y = get_data_loader(image_type='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper scale function\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    # scale from 0-1 to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper conv function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "    \n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = conv(3, conv_dim, 4, batch_norm=False) # x, y = 64, depth 64\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4) # (32, 32, 128)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4) # (16, 16, 256)\n",
    "        self.conv4 = conv(conv_dim*4, conv_dim*8, 4) # (8, 8, 512)\n",
    "        self.conv5 = conv(conv_dim*8, 1, 4, stride=1, batch_norm=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = F.relu(self.conv4(out))\n",
    "        out = self.conv5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block class\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, conv_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv_layer1 = conv(in_channels=conv_dim, out_channels=conv_dim, \n",
    "                                kernel_size=3, stride=1, padding=1, batch_norm=True)\n",
    "        \n",
    "        self.conv_layer2 = conv(in_channels=conv_dim, out_channels=conv_dim, \n",
    "                               kernel_size=3, stride=1, padding=1, batch_norm=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1 = F.relu(self.conv_layer1(x))\n",
    "        out_2 = x + self.conv_layer2(out_1)\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper deconv function\n",
    "def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Generator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim=64, n_res_blocks=6):\n",
    "        super(CycleGenerator, self).__init__()\n",
    "\n",
    "        self.conv1 = conv(3, conv_dim, 4)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        res_layers = []\n",
    "        \n",
    "        for layer in range(n_res_blocks):\n",
    "            res_layers.append(ResidualBlock(conv_dim*4))\n",
    "        self.res_blocks = nn.Sequential(*res_layers)\n",
    "\n",
    "        self.deconv1 = deconv(conv_dim*4, conv_dim*2, 4)\n",
    "        self.deconv2 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv3 = deconv(conv_dim, 3, 4, batch_norm=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        out = self.res_blocks(out)\n",
    "\n",
    "        out = F.relu(self.deconv1(out))\n",
    "        out = F.relu(self.deconv2(out))\n",
    "        out = F.tanh(self.deconv3(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(g_conv_dim=64, d_conv_dim=64, n_res_blocks=6):\n",
    "    G_XtoY = CycleGenerator(conv_dim=g_conv_dim, n_res_blocks=n_res_blocks)\n",
    "    G_YtoX = CycleGenerator(conv_dim=g_conv_dim, n_res_blocks=n_res_blocks)\n",
    "    D_X = Discriminator(conv_dim=d_conv_dim)\n",
    "    D_Y = Discriminator(conv_dim=d_conv_dim)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        G_XtoY.to(device)\n",
    "        G_YtoX.to(device)\n",
    "        D_X.to(device)\n",
    "        D_Y.to(device)\n",
    "        print('Models moved to GPU.')\n",
    "    else:\n",
    "        print('Only CPU available.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU available.\n"
     ]
    }
   ],
   "source": [
    "G_XtoY, G_YtoX, D_X, D_Y = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_mse_loss(D_out):\n",
    "    return torch.mean((D_out-1)**2)\n",
    "\n",
    "def fake_mse_loss(D_out):\n",
    "    return torch.mean(D_out**2)\n",
    "\n",
    "def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):\n",
    "    reconstr_loss = torch.mean(torch.abs(real_im - reconstructed_im))\n",
    "    return lambda_weight*reconstr_loss    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr=0.0001\n",
    "beta1=0.5\n",
    "beta2=0.999 \n",
    "\n",
    "g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  \n",
    "\n",
    "g_optimizer = optim.Adam(g_params, lr, [beta1, beta2])\n",
    "d_x_optimizer = optim.Adam(D_X.parameters(), lr, [beta1, beta2])\n",
    "d_y_optimizer = optim.Adam(D_Y.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import save code\n",
    "from helpers import save_samples, save_images, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, \n",
    "                  n_epochs=1000):\n",
    "    \n",
    "    print_every=100\n",
    "    losses = []\n",
    "    \n",
    "    test_iter_X = iter(test_dataloader_X)\n",
    "    test_iter_Y = iter(test_dataloader_Y)\n",
    "\n",
    "    fixed_X = test_iter_X.next()[0]\n",
    "    fixed_Y = test_iter_Y.next()[0]\n",
    "    fixed_X = scale(fixed_X) \n",
    "    fixed_Y = scale(fixed_Y)\n",
    "\n",
    "    iter_X = iter(dataloader_X)\n",
    "    iter_Y = iter(dataloader_Y)\n",
    "    batches_per_epoch = min(len(iter_X), len(iter_Y))\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        if epoch % batches_per_epoch == 0:\n",
    "            iter_X = iter(dataloader_X)\n",
    "            iter_Y = iter(dataloader_Y)\n",
    "\n",
    "        images_X, _ = iter_X.next()\n",
    "        images_X = scale(images_X) \n",
    "\n",
    "        images_Y, _ = iter_Y.next()\n",
    "        images_Y = scale(images_Y)\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        images_X = images_X.to(device)\n",
    "        images_Y = images_Y.to(device)\n",
    "\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATORS\n",
    "        # ============================================\n",
    "\n",
    "        d_x_optimizer.zero_grad()\n",
    "        out_x = D_X(images_X)\n",
    "        D_X_real_loss = real_mse_loss(out_x)\n",
    "        fake_X = G_YtoX(images_Y)\n",
    "        out_x = D_X(fake_X)\n",
    "        D_X_fake_loss = fake_mse_loss(out_x)\n",
    "        d_x_loss = D_X_real_loss + D_X_fake_loss\n",
    "        d_x_loss.backward()\n",
    "        d_x_optimizer.step()\n",
    "\n",
    "        d_y_optimizer.zero_grad()\n",
    "        out_y = D_Y(images_Y)\n",
    "        D_Y_real_loss = real_mse_loss(out_y)\n",
    "        fake_Y = G_XtoY(images_X)\n",
    "        out_y = D_Y(fake_Y)\n",
    "        D_Y_fake_loss = fake_mse_loss(out_y)\n",
    "\n",
    "        d_y_loss = D_Y_real_loss + D_Y_fake_loss\n",
    "        d_y_loss.backward()\n",
    "        d_y_optimizer.step()\n",
    "\n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATORS\n",
    "        # =========================================\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        fake_X = G_YtoX(images_Y)\n",
    "        out_x = D_X(fake_X)\n",
    "        g_YtoX_loss = real_mse_loss(out_x)\n",
    "        \n",
    "        reconstructed_Y = G_XtoY(fake_X)\n",
    "        reconstructed_y_loss = cycle_consistency_loss(images_Y, reconstructed_Y, lambda_weight=10)\n",
    "\n",
    "        fake_Y = G_XtoY(images_X)\n",
    "        out_y = D_Y(fake_Y)\n",
    "        g_XtoY_loss = real_mse_loss(out_y)\n",
    "        \n",
    "        reconstructed_X = G_YtoX(fake_Y)\n",
    "        reconstructed_x_loss = cycle_consistency_loss(images_X, reconstructed_X, lambda_weight=10)\n",
    "        \n",
    "        g_total_loss = g_YtoX_loss + g_XtoY_loss + reconstructed_y_loss + reconstructed_x_loss\n",
    "        g_total_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print log\n",
    "        if epoch % print_every == 0:\n",
    "            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'.format(\n",
    "                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "\n",
    "            \n",
    "        # Save\n",
    "        sample_every=500\n",
    "        if epoch % sample_every == 0:\n",
    "            G_YtoX.eval() \n",
    "            G_XtoY.eval()\n",
    "            save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=64)\n",
    "#             if epoch == n_epochs:\n",
    "#                 save_images(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=64, option='fake')\n",
    "            G_YtoX.train()\n",
    "            G_XtoY.train()\n",
    "\n",
    "        checkpoint_every=1000\n",
    "        if epoch % checkpoint_every == 0:\n",
    "            checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  100/ 5000] | d_X_loss: 0.3022 | d_Y_loss: 0.3660 | g_total_loss: 3.7217\n",
      "Epoch [  200/ 5000] | d_X_loss: 0.2672 | d_Y_loss: 0.3592 | g_total_loss: 2.9795\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5000 \n",
    "\n",
    "losses = training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
